{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from src.utilities import vocabulary, CONTEXT_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(file_path: str='data/names.txt') -> list:\n",
    "    \"\"\"Function that reads the raw data and outputs list of words\"\"\"\n",
    "    return open(file_path, 'r').read().splitlines()\n",
    "\n",
    "def word2vec(word: str, vocabulary: list=vocabulary) -> list:\n",
    "    \"\"\"Function that transforms passed word into a vector of indicies using input vocabulary\"\"\"\n",
    "    return [vocabulary.index(let) for let in word]\n",
    "\n",
    "def sample_train(words: list):\n",
    "    \"\"\"Function beaks down a random word from passed list into train and target samples\"\"\"\n",
    "    word = word2vec(words[np.random.choice(range(len(words)))] + '.')\n",
    "    X = []; y = []\n",
    "    for n, ch in enumerate(word[:-1]):\n",
    "        X.append(ch); y.append(word[n+1])\n",
    "    return torch.tensor(X), torch.tensor(y).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(torch.nn.Module):\n",
    "    def __init__(self, vocab_len: int, embedding_dim: int, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.emb = torch.nn.Embedding(num_embeddings=vocab_len, embedding_dim=embedding_dim)\n",
    "        self.lstm = torch.nn.LSTMCell(embedding_dim, self.hidden_size)\n",
    "        self.lin = torch.nn.Linear(hidden_size, vocab_len)\n",
    "\n",
    "    def forward(self, char: torch.Tensor, hidden_state: torch.Tensor, cell_state: torch.Tensor):\n",
    "        \"\"\"Applies all the network layers to the passed character encoded as a number\"\"\"\n",
    "        embedding = self.emb(char)\n",
    "        hidden_state, cell_state = self.lstm(embedding, (hidden_state, cell_state))\n",
    "        output = self.lin(hidden_state)\n",
    "        return output, hidden_state, cell_state\n",
    "    \n",
    "    def init_zero_state(self) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Initiates dummy hidden and cell states for an lstm cell\"\"\"\n",
    "        zero_hidden_state = torch.zeros(self.hidden_size)\n",
    "        zero_cell_state = torch.zeros(self.hidden_size)\n",
    "        return zero_hidden_state, zero_cell_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(vocab_len=len(vocabulary),\n",
    "          embedding_dim=20,\n",
    "          hidden_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = get_words()\n",
    "rnn_input, target = sample_train(words)\n",
    "\n",
    "def train_epoch (rnn_input, target, rnn, optimiser):\n",
    "    loss = 0\n",
    "    hidden_state, cell_state = rnn.init_zero_state()\n",
    "    for char, tar in zip(rnn_input, target):\n",
    "        output, hidden_state, cell_state = rnn.forward(char=char,\n",
    "                                                       hidden_state=hidden_state,\n",
    "                                                       cell_state=cell_state)\n",
    "        loss += torch.nn.functional.cross_entropy(output, tar.long())\n",
    "    \n",
    "    epoch_loss = loss / target.shape[0] \n",
    "    optimiser.zero_grad()\n",
    "    epoch_loss.backward()\n",
    "    optimiser.step()\n",
    "    return rnn, epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(RNN(\n",
       "   (emb): Embedding(27, 20)\n",
       "   (lstm): LSTMCell(20, 128)\n",
       "   (lin): Linear(in_features=128, out_features=27, bias=True)\n",
       " ),\n",
       " tensor(2.6938, grad_fn=<DivBackward0>))"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt = torch.optim.Adam(rnn.parameters())\n",
    "train_epoch(rnn_input, target, rnn, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: False\n",
       "    lr: 0.001\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_train(words: list):\n",
    "#     \"\"\"Function beaks down every word from passed list into train and target samples\"\"\"\n",
    "#     X = []; y=[]\n",
    "#     for word in words:\n",
    "#         context = '.' * CONTEXT_LEN\n",
    "#         for ch in word + '.':\n",
    "#             X.append(word2vec(context)); y.append(word2vec(ch))\n",
    "#             context = context[1:] + ch\n",
    "#     return torch.tensor(X), torch.tensor(y).float()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_in_ksp_new",
   "language": "python",
   "name": "rl_in_ksp_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
