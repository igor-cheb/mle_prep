{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from src.utilities import vocabulary, CONTEXT_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(file_path: str='data/names.txt') -> list:\n",
    "    \"\"\"Function that reads the raw data and outputs list of words\"\"\"\n",
    "    return open(file_path, 'r').read().splitlines()\n",
    "\n",
    "def word2vec(word: str, vocabulary: list=vocabulary) -> list:\n",
    "    \"\"\"Function that transforms passed word into a vector of indicies using input vocabulary\"\"\"\n",
    "    return [vocabulary.index(let) for let in word]\n",
    "\n",
    "def sample_train(words: list):\n",
    "    \"\"\"Function beaks down a random word from passed list into train and target samples\"\"\"\n",
    "    word = word2vec(words[np.random.choice(range(len(words)))] + '.')\n",
    "    X = []; y = []\n",
    "    for n, ch in enumerate(word[:-1]):\n",
    "        X.append(ch); y.append(word[n+1])\n",
    "    return torch.tensor(X), torch.tensor(y).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = get_words()\n",
    "input, target = sample_train(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([12,  9,  4,  9,  1])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9., 4., 9., 1., 0.])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(torch.nn.Module):\n",
    "    def __init__(self, vocab_len: int, embedding_dim: int, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.emb = torch.nn.Embedding(num_embeddings=vocab_len, embedding_dim=embedding_dim)\n",
    "        self.lstm = torch.nn.LSTMCell(embedding_dim, self.hidden_size)\n",
    "        self.lin = torch.nn.Linear(hidden_size, vocab_len)\n",
    "\n",
    "    def forward(self, char: torch.Tensor, hidden_state: torch.Tensor, cell_state: torch.Tensor):\n",
    "        \"\"\"Applies all the network layers to the passed character encoded as a number\"\"\"\n",
    "        embedding = self.emb(char)\n",
    "        hidden_state, cell_state = self.lstm(embedding, (hidden_state, cell_state))\n",
    "        output = self.lin(hidden_state)\n",
    "        return output, hidden_state, cell_state\n",
    "    \n",
    "    def init_zero_state(self) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Initiates dummy hidden and cell states for an lstm cell\"\"\"\n",
    "        zero_hidden_state = torch.zeros(self.hidden_size)\n",
    "        zero_cell_state = torch.zeros(self.hidden_size)\n",
    "        return zero_hidden_state, zero_cell_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(vocab_len=len(vocabulary),\n",
    "          embedding_dim=20,\n",
    "          hidden_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.0453, -0.0190,  0.0697, -0.0252, -0.0370,  0.0077, -0.1055, -0.0464,\n",
       "         -0.0520, -0.0313, -0.1233, -0.0230,  0.1307,  0.0703, -0.0355,  0.0156,\n",
       "         -0.0300, -0.1042, -0.0973,  0.1073, -0.0235, -0.0679, -0.0452,  0.1162,\n",
       "         -0.0536,  0.0659,  0.1008], grad_fn=<AddBackward0>),\n",
       " tensor([-0.0027,  0.0092, -0.0104,  0.0678, -0.0068,  0.1505, -0.0362, -0.0111,\n",
       "         -0.0511,  0.1982, -0.0137,  0.0637,  0.0907,  0.0112, -0.0136, -0.0872,\n",
       "         -0.0179, -0.0207, -0.0629, -0.0434,  0.0057,  0.0419,  0.0539,  0.0109,\n",
       "          0.0208, -0.0333,  0.0100,  0.0956, -0.0188,  0.1072, -0.0160, -0.0899,\n",
       "          0.0275, -0.0763,  0.0459,  0.0358,  0.0462,  0.0246,  0.0239, -0.0227,\n",
       "         -0.0721,  0.0820,  0.0992, -0.0416,  0.0157,  0.0309,  0.0597, -0.0478,\n",
       "          0.0129,  0.0224, -0.0457, -0.0412, -0.0260,  0.0240, -0.0971,  0.0139,\n",
       "         -0.1770, -0.0113, -0.0146,  0.0628,  0.0128, -0.0851,  0.0672, -0.0397,\n",
       "         -0.0373, -0.0523,  0.0299, -0.0178, -0.0162, -0.0054,  0.0354, -0.0290,\n",
       "          0.0565,  0.0599, -0.0062,  0.1075, -0.0807, -0.0275, -0.0209,  0.0147,\n",
       "         -0.0346, -0.0297, -0.0172,  0.0374,  0.0193, -0.0536, -0.0473, -0.0381,\n",
       "          0.0304,  0.0033, -0.1145,  0.0101, -0.0004,  0.0290, -0.0557, -0.0004,\n",
       "         -0.0396, -0.0042, -0.0621,  0.0297,  0.0261,  0.0341, -0.0071, -0.0525,\n",
       "         -0.0196,  0.0343,  0.0418, -0.0149,  0.0416, -0.0111,  0.0201, -0.0488,\n",
       "         -0.0417,  0.0306, -0.0458, -0.0008, -0.0167, -0.0516, -0.0003, -0.0118,\n",
       "         -0.0600, -0.0196,  0.0513,  0.0743,  0.0740, -0.0102,  0.0305,  0.0745],\n",
       "        grad_fn=<SqueezeBackward1>),\n",
       " tensor([-0.0055,  0.0176, -0.0254,  0.1362, -0.0136,  0.2992, -0.0724, -0.0289,\n",
       "         -0.0908,  0.3644, -0.0262,  0.1123,  0.2071,  0.0204, -0.0267, -0.2009,\n",
       "         -0.0304, -0.0422, -0.1441, -0.0762,  0.0126,  0.0930,  0.1219,  0.0241,\n",
       "          0.0488, -0.0726,  0.0187,  0.1819, -0.0348,  0.2076, -0.0334, -0.2011,\n",
       "          0.0594, -0.1284,  0.0773,  0.0803,  0.0884,  0.0438,  0.0542, -0.0576,\n",
       "         -0.1459,  0.1611,  0.2068, -0.0876,  0.0295,  0.0545,  0.1161, -0.1022,\n",
       "          0.0342,  0.0412, -0.0865, -0.0826, -0.0515,  0.0414, -0.2258,  0.0257,\n",
       "         -0.2797, -0.0195, -0.0301,  0.0984,  0.0278, -0.1820,  0.1460, -0.0865,\n",
       "         -0.0827, -0.1005,  0.0505, -0.0344, -0.0323, -0.0096,  0.0681, -0.0541,\n",
       "          0.1286,  0.1380, -0.0137,  0.2484, -0.1577, -0.0521, -0.0479,  0.0292,\n",
       "         -0.0697, -0.0586, -0.0314,  0.0672,  0.0430, -0.1211, -0.0859, -0.0932,\n",
       "          0.0600,  0.0068, -0.2145,  0.0208, -0.0010,  0.0586, -0.1148, -0.0007,\n",
       "         -0.0903, -0.0080, -0.1015,  0.0653,  0.0562,  0.0615, -0.0167, -0.1019,\n",
       "         -0.0394,  0.0683,  0.0720, -0.0282,  0.0876, -0.0235,  0.0384, -0.0899,\n",
       "         -0.0835,  0.0629, -0.1056, -0.0014, -0.0279, -0.0929, -0.0006, -0.0288,\n",
       "         -0.1581, -0.0373,  0.0954,  0.1528,  0.1363, -0.0214,  0.0562,  0.1303],\n",
       "        grad_fn=<SqueezeBackward1>))"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_hidden_state, zero_cell_state = rnn.init_zero_state()\n",
    "rnn.forward(char=input[0],\n",
    "            hidden_state=zero_hidden_state,\n",
    "            cell_state=zero_cell_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_train(words: list):\n",
    "#     \"\"\"Function beaks down every word from passed list into train and target samples\"\"\"\n",
    "#     X = []; y=[]\n",
    "#     for word in words:\n",
    "#         context = '.' * CONTEXT_LEN\n",
    "#         for ch in word + '.':\n",
    "#             X.append(word2vec(context)); y.append(word2vec(ch))\n",
    "#             context = context[1:] + ch\n",
    "#     return torch.tensor(X), torch.tensor(y).float()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_in_ksp_new",
   "language": "python",
   "name": "rl_in_ksp_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
